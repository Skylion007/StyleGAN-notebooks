{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_s8h-ilzHQc"
   },
   "source": [
    "# StyleGAN Anime Sliders\n",
    "This notebook demonstrate how to learn and extract controllable directions from [ThisAnimeDoesNotExist](https://thisanimedoesnotexist.ai/). This takes a pretrained StyleGAN and uses [DeepDanbooru](https://github.com/KichangKim/DeepDanbooru) to extract various labels from a number of samples. It then uses those labels to learn various attributes which are controllable with sliders.\n",
    "Credits of the modeling goes to the Aydao and the Tensorfork community.\n",
    "\n",
    "Topics covered include:\n",
    "1. Generating images\n",
    "1. Tagging images and learning directions from the latent space\n",
    "1. Style Transfer\n",
    "1. How to Project Images to the Latent Space\n",
    "1. Steering with CLIP (coming soon) \n",
    "\n",
    "This is a work in progress. Stay tuned for more updates to the colab.\n",
    "\n",
    "By Aaron Gokaslan ([Skyli0n](https://twitter.com/SkyLi0n)) 2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PzDuIoMcqfBT",
    "outputId": "0f1ebf28-2e4c-448f-b21e-ee50a9971d45"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "!pip install -U moviepy\n",
    "!pip install cleanlab\n",
    "import proglog\n",
    "proglog.notebook()\n",
    "\n",
    "# Download the code\n",
    "%cd /content/\n",
    "!pip install git+https://github.com/KichangKim/DeepDanbooru --no-deps\n",
    "\n",
    "!mkdir -p /content/models\n",
    "%cd /content/models\n",
    "!wget -nc https://github.com/KichangKim/DeepDanbooru/releases/download/v3-20200101-sgd-e30/deepdanbooru-v3-20200101-sgd-e30.zip -O deepdanbooru.zip\n",
    "# Try the V4 version \n",
    "#!wget -nc https://github.com/KichangKim/DeepDanbooru/releases/download/v4-20200814-sgd-e30/deepdanbooru-v4-20200814-sgd-e30.zip -O deepdanbooru.zip\n",
    "!unzip -n deepdanbooru.zip\n",
    "%cd /content\n",
    "\n",
    "!git clone https://github.com/shawwn/stylegan2 -b estimator /content/stylegan2 --depth 1\n",
    "%cd /content/stylegan2\n",
    "\n",
    "!nvcc test_nvcc.cu -o test_nvcc -run\n",
    "\n",
    "print('Tensorflow version: {}'.format(tf.__version__) )\n",
    "!nvidia-smi -L\n",
    "print('GPU Identified at: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtNEWjw_yifT"
   },
   "source": [
    "## Load the StyleGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwVXBFaSuoIU",
    "outputId": "b2562b7f-668f-4294-a34b-79038785f0c9"
   },
   "outputs": [],
   "source": [
    "%cd /content/stylegan2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "import re\n",
    "import sys\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from PIL import Image, ImageDraw\n",
    "import imageio\n",
    "\n",
    "import pretrained_networks\n",
    "\n",
    "import hashlib \n",
    "\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "\n",
    "url = 'https://drive.google.com/open?id=1A-E_E32WAtTHRlOzjhhYhyyBDXLJN9_H'\n",
    "model_id = url.replace('https://drive.google.com/open?id=', '')\n",
    "\n",
    "network_pkl = '/content/models/model_%s.pkl' % model_id#(hashlib.md5(model_id.encode()).hexdigest())\n",
    "gdd.download_file_from_google_drive(file_id=model_id, dest_path=network_pkl)\n",
    "\n",
    "\n",
    "print('Loading networks from \"%s\"...' % network_pkl)\n",
    "_G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
    "noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gBJsWpPy525"
   },
   "source": [
    "## Display Utility\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0H9EaMQkWb5"
   },
   "outputs": [],
   "source": [
    "from IPython import display as ipythondisplay\n",
    "import io\n",
    "import os\n",
    "import base64\n",
    "import moviepy\n",
    "from IPython.display import HTML\n",
    "\n",
    "def show_video(vid):\n",
    "  ext = os.path.splitext(vid)[-1][1:]\n",
    "  height = 400\n",
    "  tmp_video_file = os.path.join(os.path.dirname(vid), 'tmp_' + os.path.basename(vid))\n",
    "  with moviepy.editor.VideoFileClip(vid, target_resolution=(height, None)) as clip:\n",
    "    clip.write_videofile(tmp_video_file, preset='veryslow')\n",
    "  video = io.open(vid, 'r+b').read()\n",
    "  os.remove(tmp_video_file)\n",
    "  ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "              loop controls style=\"height: {2}px;\">\n",
    "              <source src=\"data:video/{1}';base64,{0}\" type=\"video/{1}\" />\n",
    "              </video>'''.format(base64.b64encode(video).decode('ascii'), ext, height)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5UvIjKDy9lt"
   },
   "source": [
    "## Misc Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zxbhe4uLvF_a"
   },
   "outputs": [],
   "source": [
    "# Useful utility functions...\n",
    "\n",
    "# Generates a list of images, based on a list of latent vectors (Z), and a list (or a single constant) of truncation_psi's.\n",
    "def generate_images_in_w_space(dlatents, truncation_psi, randomize_noise=False, show_progress=True):\n",
    "    Gs_kwargs = dnnlib.EasyDict()\n",
    "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    Gs_kwargs.randomize_noise = randomize_noise\n",
    "    Gs_kwargs.truncation_psi = truncation_psi\n",
    "    dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
    "\n",
    "    if not show_progress:\n",
    "      log_progress_ = lambda args, **kwargs: args\n",
    "    else:\n",
    "      log_progress_ = log_progress\n",
    "\n",
    "    imgs = []\n",
    "    for row, dlatent in log_progress_(enumerate(dlatents), name = \"Generating images\"):\n",
    "        row_dlatents = (dlatent[np.newaxis] - dlatent_avg) * np.reshape(truncation_psi, [-1, 1, 1]) + dlatent_avg\n",
    "        row_images = Gs.components.synthesis.run(row_dlatents,  **Gs_kwargs)\n",
    "        imgs.extend([PIL.Image.fromarray(r, 'RGB') for r in row_images])\n",
    "    return imgs       \n",
    "\n",
    "def generate_images(zs, truncation_psi, randomize_noise=False, show_progress=True):\n",
    "    Gs_kwargs = dnnlib.EasyDict()\n",
    "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    Gs_kwargs.randomize_noise = randomize_noise\n",
    "    if not isinstance(truncation_psi, list):\n",
    "        truncation_psi = [truncation_psi] * len(zs)\n",
    "        \n",
    "    if not show_progress:\n",
    "      log_progress_ = lambda args, **kwargs: args\n",
    "    else:\n",
    "      log_progress_ = log_progress\n",
    "    imgs = []\n",
    "    for z_idx, z in log_progress_(enumerate(zs), size = len(zs), name = \"Generating images\"):\n",
    "        Gs_kwargs.truncation_psi = truncation_psi[z_idx]\n",
    "        noise_rnd = np.random.RandomState(1) # fix noise\n",
    "        tflib.set_vars({var: noise_rnd.randn(*var.shape.as_list()) for var in noise_vars}) # [height, width]\n",
    "        images = Gs.run(z, None, **Gs_kwargs) # [minibatch, height, width, channel]\n",
    "        for image in images:\n",
    "          imgs.append(PIL.Image.fromarray(image, 'RGB'))\n",
    "    return imgs\n",
    "\n",
    "def generate_zs_from_seeds(seeds):\n",
    "    zs = []\n",
    "    for seed_idx, seed in enumerate(seeds):\n",
    "        rnd = np.random.RandomState(seed)\n",
    "        z = rnd.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n",
    "        zs.append(z)\n",
    "    return zs\n",
    "\n",
    "# Generates a list of images, based on a list of seed for latent vectors (Z), and a list (or a single constant) of truncation_psi's.\n",
    "def generate_images_from_seeds(seeds, truncation_psi):\n",
    "    return generate_images(generate_zs_from_seeds(seeds), truncation_psi)\n",
    "\n",
    "def saveImgs(imgs, location):\n",
    "  for idx, img in log_progress(enumerate(imgs), size = len(imgs), name=\"Saving images\"):\n",
    "    file = location+ str(idx) + \".png\"\n",
    "    img.save(file)\n",
    "\n",
    "def imshow(a, format='png', jpeg_fallback=True):\n",
    "  a = np.asarray(a, dtype=np.uint8)\n",
    "  str_file = BytesIO()\n",
    "  PIL.Image.fromarray(a).save(str_file, format)\n",
    "  im_data = str_file.getvalue()\n",
    "  try:\n",
    "    disp = IPython.display.display(IPython.display.Image(im_data))\n",
    "  except IOError:\n",
    "    if jpeg_fallback and format != 'jpeg':\n",
    "      print ('Warning: image was too large to display in format \"{}\"; '\n",
    "             'trying jpeg instead.').format(format)\n",
    "      return imshow(a, format='jpeg')\n",
    "    else:\n",
    "      raise\n",
    "  return disp\n",
    "\n",
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(a)\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "        \n",
    "def clamp(x, minimum, maximum):\n",
    "    return max(minimum, min(x, maximum))\n",
    "    \n",
    "def drawLatent(image,latents,x,y,x2,y2, color=(255,0,0,100)):\n",
    "  buffer = PIL.Image.new('RGBA', image.size, (0,0,0,0))\n",
    "   \n",
    "  draw = ImageDraw.Draw(buffer)\n",
    "  cy = (y+y2)/2\n",
    "  draw.rectangle([x,y,x2,y2],fill=(255,255,255,180), outline=(0,0,0,180))\n",
    "  for i in range(len(latents)):\n",
    "    mx = x + (x2-x)*(float(i)/len(latents))\n",
    "    h = (y2-y)*latents[i]*0.1\n",
    "    h = clamp(h,cy-y2,y2-cy)\n",
    "    draw.line((mx,cy,mx,cy+h),fill=color)\n",
    "  return PIL.Image.alpha_composite(image,buffer)\n",
    "             \n",
    "  \n",
    "def createImageGrid(images, scale=0.25, rows=1):\n",
    "   w,h = images[0].size\n",
    "   w = int(w*scale)\n",
    "   h = int(h*scale)\n",
    "   height = rows*h\n",
    "   cols = ceil(len(images) / rows)\n",
    "   width = cols*w\n",
    "   canvas = PIL.Image.new('RGBA', (width,height), 'white')\n",
    "   for i,img in enumerate(images):\n",
    "     img = img.resize((w,h), PIL.Image.ANTIALIAS)\n",
    "     canvas.paste(img, (w*(i % cols), h*(i // cols))) \n",
    "   return canvas\n",
    "\n",
    "def convertZtoW(latent, truncation_psi=0.7, truncation_cutoff=9):\n",
    "  dlatent = Gs.components.mapping.run(latent, None) # [seed, layer, component]\n",
    "  \n",
    "  dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
    "  for j in range(len(dlatent)):\n",
    "    for i in range(truncation_cutoff):\n",
    "      dlatent[j][i] = (dlatent[j][i]-dlatent_avg)*truncation_psi + dlatent_avg\n",
    "      \n",
    "  return dlatent\n",
    "\n",
    "def interpolate(zs, steps):\n",
    "   out = []\n",
    "   for i in range(len(zs)-1):\n",
    "    for index in range(steps):\n",
    "     fraction = index/float(steps) \n",
    "     out.append(zs[i+1]*fraction + zs[i]*(1-fraction))\n",
    "   return out\n",
    "\n",
    "# Taken from https://github.com/alexanderkuk/log-progress\n",
    "def log_progress(sequence, every=1, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zar5nd5HDH1V"
   },
   "source": [
    "# Learn the Attribute Labels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecdNFpyk-Vdo"
   },
   "source": [
    "## Uses the DeepDanbooru Classifier ontop of the anime GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2c2GBGyo1dq"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('/content/models/model-resnet_custom_v3.h5', compile=True)\n",
    "#model = tf.keras.models.load_model('/content/models/model-resnet_custom_v4.h5', compile=True)\n",
    "\n",
    "def load_tags(tags_path):\n",
    "    with open(tags_path, 'r') as tags_stream:\n",
    "        tags = [tag for tag in (tag.strip() for tag in tags_stream) if tag]\n",
    "        return tags\n",
    "\n",
    "tags = np.asarray(load_tags('/content/models/tags.txt'))  \n",
    "import cv2\n",
    "import io\n",
    "import deepdanbooru.data\n",
    "import deepdanbooru as dd \n",
    "from deepdanbooru.commands.evaluate import evaluate_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20enF-XPzLqE"
   },
   "source": [
    "## Generate the labels\r\n",
    "This next part generates a bunch of images and tags them with the DeepDanbooru classifier. This is done in two seperate ways. We can either map the Z or the W latents to the binary attribute labels. The W latents are the \"style\" latents of stylegan while Z is the more classical GAN latent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7T7nNrOmvXu"
   },
   "outputs": [],
   "source": [
    "  \n",
    "tags = np.asarray(load_tags('/content/models/tags.txt'))  \n",
    "import cv2\n",
    "import deepdanbooru as dd\n",
    "from tqdm.auto import trange\n",
    "DD_INPUT_SIZE = 512\n",
    "\n",
    "def generate_and_tag(rnd):\n",
    "    z = rnd.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n",
    "    image_orig = generate_images([z], 0.5, randomize_noise=True, show_progress=False)[0]#(seeds, 0.5)[0]\n",
    "    image = np.asarray(image_orig)\n",
    "    image = cv2.resize(image, (DD_INPUT_SIZE, DD_INPUT_SIZE))\n",
    "    image = np.squeeze(image)\n",
    "\n",
    "    image = dd.image.transform_and_pad_image(image, DD_INPUT_SIZE, DD_INPUT_SIZE)\n",
    "\n",
    "    image = np.expand_dims(image, 0)\n",
    "    prediction = model.predict(image)[0]\n",
    "    tag_ids = np.where(prediction > 0.5)\n",
    "\n",
    "    return z, prediction\n",
    "\n",
    "\n",
    "def generate_and_tagb(rnd, batch):\n",
    "    z = rnd.randn(batch, *Gs.input_shape[1:]) # [minibatch, component]\n",
    "\n",
    "    images = generate_images([z], 0.5, randomize_noise=True, show_progress=False)#(seeds, 0.5)[0]\n",
    "    images_pred = []\n",
    "    for image_orig in images:\n",
    "      image = np.asarray(image_orig)\n",
    "\n",
    "      image = cv2.resize(image, (DD_INPUT_SIZE, DD_INPUT_SIZE), interpolation=cv2.INTER_AREA)\n",
    "      image = np.squeeze(image)\n",
    "\n",
    "      image = dd.image.transform_and_pad_image(image, DD_INPUT_SIZE, DD_INPUT_SIZE)\n",
    "      image = np.expand_dims(image, 0)\n",
    "      images_pred.append(image)\n",
    "  \n",
    "    predictions = model.predict(np.concatenate(images_pred))\n",
    "\n",
    "    return z, predictions\n",
    "\n",
    "\n",
    "def gen_and_tag(n, seed):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    output_x_data = []\n",
    "    output_y_data = []\n",
    "    batch = 16\n",
    "    for i in trange(n//batch):\n",
    "      zs, predictions = generate_and_tagb(rnd, batch)\n",
    "      for z, prediction in zip(zs, predictions):\n",
    "        output_x_data.append(z)\n",
    "        output_y_data.append(prediction)\n",
    "    if n % batch != 0:\n",
    "      zs, predictions = generate_and_tagb(rnd, n % batch)\n",
    "      for z, prediction in zip(zs, predictions):\n",
    "        output_x_data.append(z)\n",
    "        output_y_data.append(prediction)\n",
    "    \n",
    "    return np.squeeze(np.array(output_x_data)), np.array(output_y_data)\n",
    "\n",
    "def generate_and_tag_w(rnd, batch):\n",
    "    z = rnd.randn(batch, *Gs.input_shape[1:]) # [minibatch, component]\n",
    "    \n",
    "    w = convertZtoW(z, truncation_psi=1.0, truncation_cutoff=Gs.components.mapping.output_shape[1])\n",
    "    images = generate_images_in_w_space(w, 1.0, randomize_noise=True, show_progress=False)\n",
    "    images_pred = []\n",
    "    for image_orig in images:\n",
    "      image = np.asarray(image_orig)\n",
    "\n",
    "      image = cv2.resize(image, (DD_INPUT_SIZE, DD_INPUT_SIZE), interpolation=cv2.INTER_AREA)\n",
    "      image = np.squeeze(image)\n",
    "    \n",
    "      image = dd.image.transform_and_pad_image(image, DD_INPUT_SIZE, DD_INPUT_SIZE)\n",
    "      image = np.expand_dims(image, 0)\n",
    "      images_pred.append(image)\n",
    "  \n",
    "    predictions = model.predict(np.concatenate(images_pred))\n",
    "    return w, predictions\n",
    "\n",
    "\n",
    "def gen_and_tag_w(n, seed):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    output_x_data = []\n",
    "    output_y_data = []\n",
    "    batch = 16\n",
    "    for i in trange(n//batch):\n",
    "      zs, predictions = generate_and_tag_w(rnd, batch)\n",
    "      for z, prediction in zip(zs, predictions):\n",
    "        output_x_data.append(z)\n",
    "        output_y_data.append(prediction)\n",
    "    if n % batch != 0:\n",
    "      zs, predictions = generate_and_tag_w(rnd, n % batch)\n",
    "      for z, prediction in zip(zs, predictions):\n",
    "        output_x_data.append(z)\n",
    "        output_y_data.append(prediction)\n",
    "    \n",
    "    return np.array(output_x_data), np.array(output_y_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7gd696AzXGM"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRCnubswzj2Z"
   },
   "source": [
    "## Logistic Regression on the Labels and the Colleted Latents\r\n",
    "\r\n",
    "This function trains an L2 logistic regression to try to latent W or Z latents to binary **attributes** of that classified earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2sgH9cGu72K"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from cleanlab.classification import LearningWithNoisyLabels\n",
    "\n",
    "def get_directions_dict(X_data_arr, y_data_arr):\n",
    "  threshold = 0.5 # The threshold for number of positive classifications needed to learn an attribute\n",
    "  valid_tags = (np.sum(y_data_arr > threshold, axis=0) > 25).nonzero()[0]\n",
    "  print('Num of Valid Tags: %d' % len(valid_tags))\n",
    "  directions_dict = dict()\n",
    "  clf = SGDClassifier('log', class_weight='balanced', penalty='l2', n_iter_no_change=20, early_stopping=True)\n",
    "  for j in trange(len(valid_tags)):\n",
    "    i = valid_tags[j]\n",
    "    # This decides if there are enough positive classifications to learn the attribute\n",
    "    if np.min(np.bincount(y_data_arr[:, i] >= threshold)) >= 2: #and not np.all(y_data_arr[:, i] >= threshold) and not np.all(y_data_arr[:, i] < threshold): \n",
    "      clf = clf.fit(X_data_arr.reshape(X_data_arr.shape[0], -1), y_data_arr[:,i] >= threshold)\n",
    "      new_direction = clf.coef_.reshape(X_data_arr[0].shape)\n",
    "      score = clf.score(X_data_arr.reshape(X_data_arr.shape[0], -1), y_data_arr[:,i] >= threshold)\n",
    "      if score >= 0.5:\n",
    "        directions_dict[tags[i]] = new_direction\n",
    "  return directions_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8UxJtojtr2v"
   },
   "source": [
    "## Learn Z Embedding for Attributes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "6e16884b7248490eb735d1a503887c82",
      "47d4c9102f204c9f80064c520fcf4b55",
      "024bb597f58a4517abbccfd6a256b069",
      "17f345dba1964906a053889ba1a5fa22",
      "8a5fb00e634e4f80b6ab621a04fcec7a",
      "8e05b9f11f1f4529a59d47c67f30700b",
      "7e2d271c6d7d40e7bee7dee2da33e365",
      "66276cce475f4aec901cd234f19e683f"
     ]
    },
    "id": "W28cfykbqXbF",
    "outputId": "032bdbb8-1ab9-49f7-b0e5-7b5fb95d8857"
   },
   "outputs": [],
   "source": [
    "# Generates 2500 samples. Reducing it will make this step faster\n",
    "X_data_arr, y_data_arr = gen_and_tag(2500, 1234)\n",
    "np.save('X_data', X_data_arr)\n",
    "np.save('y_data', y_data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZO11pWeCUcR"
   },
   "outputs": [],
   "source": [
    "X_data_arr = np.load('X_data.npy')\r\n",
    "y_data_arr = np.load('y_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmruQJk8uOu9",
    "outputId": "34b96688-0da3-4e98-c114-0dba6f9dbc3a"
   },
   "outputs": [],
   "source": [
    "attribute_counter = dict()\n",
    "for t, d in enumerate((y_data_arr > 0.5).sum(axis=0)):#tags[tag_ids]:\n",
    "  attribute_counter[tags[t]]=d\n",
    "\n",
    "print(sorted(attribute_counter.items(), key=lambda x:x[1], reverse=True))\n",
    "\n",
    "continuous_attribute_counter = dict()\n",
    "for t, d in enumerate(y_data_arr.sum(axis=0)):#tags[tag_ids]:\n",
    "  continuous_attribute_counter[tags[t]]=d\n",
    "\n",
    "print(sorted(continuous_attribute_counter.items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "c8d7b1e527384d5b87a3465756dcbd82",
      "6f5004c186e84d2f92ae64dc5a91deb9",
      "6b9fef36a2144f58a4b6b613847b421e",
      "c7d65e3ae21c4aa1b7cba30066e78c54",
      "f10a50c160934d958c2095649f9c0543",
      "93399ffd403d4c30b42a09b2b7ec33c1",
      "e3ff67611110459e9069726d4400705b",
      "5184f49e3b874eecb7677bcadec7334c"
     ]
    },
    "id": "QUuk2xW4t6WT",
    "outputId": "9c0fd160-1ce0-4821-a137-e637fbc09378"
   },
   "outputs": [],
   "source": [
    "# This does the actual machine learning\r\n",
    "directions_dict = get_directions_dict(X_data_arr, y_data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1IvzsuPxqCH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def move_and_show_row(latent_vector, direction, coeffs, mask, fig, ax, key):\n",
    "    for i, coeff in enumerate(coeffs):\n",
    "        new_latent_vector = latent_vector.copy()\n",
    "        new_latent_vector = ((latent_vector + coeff*direction*mask))\n",
    "        ax[i].imshow(generate_images([new_latent_vector], 1.0, randomize_noise=False, show_progress=False)[0])\n",
    "        ax[i].set_title(f'{key} Coeff: %0.1f' % coeff)\n",
    "    [x.axis('off') for x in ax]\n",
    "\n",
    "def move_and_show_dir(latent_vector, direction, coeffs, mask):\n",
    "    fig,ax = plt.subplots(1, len(coeffs), figsize=(15, 10), dpi=80)\n",
    "    for i, coeff in enumerate(coeffs):\n",
    "        new_latent_vector = latent_vector.copy()\n",
    "        new_latent_vector = ((latent_vector + coeff*direction*mask))\n",
    "        ax[i].imshow(generate_images([new_latent_vector], 1.0, randomize_noise=False, show_progress=False)[0])\n",
    "        ax[i].set_title('Coeff: %0.1f' % coeff)\n",
    "    [x.axis('off') for x in ax]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Jd7i-zNy2MZu",
    "outputId": "341d8a5d-b1b6-48ab-d2fe-a7751a0d0200"
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize(v):\n",
    "    \"\"\"Normalizes the vector v to be a unit vector\"\"\"\n",
    "    norm=np.linalg.norm(v, ord=2)\n",
    "    if norm==0:\n",
    "        norm=np.finfo(v.dtype).eps\n",
    "    return v/norm\n",
    "\n",
    "latent_vector = np.random.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n",
    "mask = np.zeros(*Gs.input_shape[1:], dtype=float)\n",
    "mask[:] = 1 # This is a mask of how much of the latents you want to actually change.\n",
    "coeffs = [-5.0, -2.5, -1.0, 0, 1.0, 2.5, 5.0] # How strongly you want to apply the learned attribute. \n",
    "#Extreme values will distort image quality\n",
    "print(len(directions_dict))\n",
    "for key in directions_dict:\n",
    "    print(key, np.argmax(directions_dict[key]))\n",
    "    move_and_show_dir(latent_vector, normalize(directions_dict[key]), coeffs, mask)#, fig, ax, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143,
     "referenced_widgets": [
      "0479d0d8a4f343dc8d639866bd3c77ff",
      "eebe2ccc374c419688cdc24ac3d1563b",
      "ed9e82b49542479f9471627e070f7361",
      "541eac5de0f141f58a4c5cfab9de9300",
      "ffbf8c411a1d41fcb95d6701d0436816",
      "7ef9631bbcfc48b9bff83c20b6bd4228",
      "f890c8b2fa5c4d8e94af3bc053a863d3",
      "2bc09e760c114bf3987957976f24fbfe",
      "1af85613f3024d0882b063b1fb4195e3",
      "8b63eb11a8594116b424d6f17e0fbba9",
      "7c4e499538ac49449d2d5e652067e178",
      "8861a29b56f74ea6962a7145215e0444"
     ]
    },
    "id": "ggbHIJWgG1WR",
    "outputId": "e707f912-94f8-46de-bbc7-3337b635b05a"
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "categories = list(directions_dict.keys())\n",
    "category_picker = widgets.Dropdown(options=categories)\n",
    "slider = widgets.IntSlider(value=2, max=4)\n",
    "coeffs = [-5, -1.5, 0, 1.5, 5]\n",
    "\n",
    "\n",
    "display(category_picker)\n",
    "display(slider)\n",
    "out = widgets.Output()\n",
    "\n",
    "latent_vector = np.random.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n",
    "\n",
    "\n",
    "button = widgets.Button(description=\"Generate!\")\n",
    "button_clear = widgets.Button(description='Clear')\n",
    "def run_button(*args):\n",
    "  coeff = coeffs[slider.value]\n",
    "  global latent_vector\n",
    "  direction = directions_dict[category_picker.value]\n",
    "  direction = normalize(direction)\n",
    "  latent_vector = ((latent_vector + coeff*direction*mask))\n",
    "  with out:\n",
    "    image = generate_images([latent_vector], 0.5, randomize_noise=True, show_progress=False)[0]\n",
    "    IPython.display.clear_output(True)\n",
    "    imshow(image)\n",
    "  \n",
    "button.on_click(run_button)\n",
    "display(button, button_clear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXp9OfSLuIJr"
   },
   "source": [
    "## Learn W Embedding for Attributes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "7084338ccad2439992526230bd0d2c2f",
      "b3e7f488c7c945a99c254219b0fcde1d",
      "8a4d6b8ff36d4b2ba5085f3936726a3f",
      "8749215b9ece462cb7388a50397ae6f8",
      "5c619161484c4ede984717cf1741f59f",
      "c2ed8dfce77846bab99b5ad94d86f8d8",
      "b85f547587274dc883a0596eeb2ef3d0",
      "2c930f6afa154048b8d82d4c2cda38dd"
     ]
    },
    "id": "-o34s9sL_I6k",
    "outputId": "d78c3a65-8fe3-4a04-fb4d-5cd3422edc12"
   },
   "outputs": [],
   "source": [
    "X_data_arr, y_data_arr = gen_and_tag_w(10000, 1234)\n",
    "print(X_data_arr.shape)\n",
    "\n",
    "np.save('X_data_w', X_data_arr)\n",
    "np.save('y_data_w', y_data_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqbMhFNmKoXc"
   },
   "outputs": [],
   "source": [
    "X_data_arr = np.load('X_data_w.npy')\n",
    "y_data_arr = np.load('y_data_w.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "a683ca10d7ae4f1397823789e636fea5",
      "806fae4b459640b38b213b228fdbe5cd",
      "d103b3139c1d4f93be32cef12483177d",
      "b925ca0272cf4049a9290ee2a03baaa9",
      "4f88bf1da38248f1a5230b61a9b6900b",
      "bb7cee79d2674adf9a818e0f1d5831ad",
      "2e17e472b23c42baa6b474b7fad89944",
      "ea5a057a109f4e759134a92978a5dd04"
     ]
    },
    "id": "MqadLVu10bVX",
    "outputId": "c6683de5-5704-4e26-b55d-f282445b3751"
   },
   "outputs": [],
   "source": [
    "directions_dict = get_directions_dict(X_data_arr, y_data_arr)\n",
    "print(len(directions_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "or8hpVbWCJrp",
    "outputId": "8e0443ec-6ad4-4442-e2d0-ee90119df44f"
   },
   "outputs": [],
   "source": [
    "attribute_counter = dict()\n",
    "for t, d in enumerate((y_data_arr > 0.5).sum(axis=0)):#tags[tag_ids]:\n",
    "  attribute_counter[tags[t]]=d\n",
    "\n",
    "print(sorted(attribute_counter.items(), key=lambda x:x[1], reverse=True))\n",
    "\n",
    "continuous_attribute_counter = dict()\n",
    "for t, d in enumerate(y_data_arr.sum(axis=0)):#tags[tag_ids]:\n",
    "  continuous_attribute_counter[tags[t]]=d\n",
    "\n",
    "print(sorted(continuous_attribute_counter.items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vXKO4nD6H8l"
   },
   "source": [
    "When learning the W embedding, the layers used will affect how will a particular attribute transfers. For certain attributes like hair and eye color, only the highest level attributes will be useful. Therefore, you would want to consider mask the last 128 attributes ```mask[-128:]``` for instance. For other attributes like body shape, pose, and scenery, you will only get good results from modifying lower level attributes.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9_B-atuSFAp3",
    "outputId": "bb765893-0609-45d6-f3d6-4713c7a90938"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def normalize(v):\n",
    "    norm=np.linalg.norm(v, ord=2)\n",
    "    if norm==0:\n",
    "        norm=np.finfo(v.dtype).eps\n",
    "    return v/norm * len(v)\n",
    "\n",
    "latent_vector = np.random.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n",
    "latent_vector = convertZtoW(latent_vector, truncation_psi=1.0)[0]\n",
    "mask = np.zeros(latent_vector.shape, dtype=float)\n",
    "\n",
    "mask[:] = 1 # Which layers you want to actually use. \n",
    "print(latent_vector.shape)\n",
    "\n",
    "def move_and_show_dir_w(latent_vector, direction, coeffs, mask):\n",
    "    direction=normalize(direction)\n",
    "    fig,ax = plt.subplots(1, len(coeffs), figsize=(15, 10), dpi=80)\n",
    "    for i, coeff in enumerate(coeffs):\n",
    "        new_latent_vector = latent_vector.copy()\n",
    "        new_latent_vector = ((latent_vector + coeff*direction*mask))\n",
    "        ax[i].imshow(generate_images_in_w_space(np.expand_dims(new_latent_vector,0), 1.0, randomize_noise=True, show_progress=False)[0])\n",
    "        ax[i].set_title('Coeff: %0.1f' % coeff)\n",
    "    [x.axis('off') for x in ax]\n",
    "    plt.show()\n",
    "\n",
    "keys = [k for k,v in sorted(continuous_attribute_counter.items(), key=lambda x:x[1], reverse=True) if k in directions_dict]\n",
    "for key in keys:\n",
    "    print(key)\n",
    "    move_and_show_dir_w(latent_vector, directions_dict[key], [-5.0, -2.5, -1.0, 0, 1.0, 2.5, 5.0], mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175,
     "referenced_widgets": [
      "eb7eead84b9d4c33a6f207a4cec45ead",
      "53e6a7f0b4b243a89eab61047de9c008",
      "f209e12be5334ccb9c0d176756404242",
      "758e742ff78a4a439057abe90c562cae",
      "67379d8955a74e96b6cdde0b82b4eecd",
      "789434a1aecc43c1ae7da1db81315561",
      "4adcfefb12e340a4bdba5c91f80bb50e",
      "8dbd8348d692469eb03e22093e5b5cc4",
      "da43bf304b6e4ed1b55f9c11271baf89",
      "2afa0364a9bf4d7da396076fd170b631",
      "c17e7d6387614f468e637d6361fcb244",
      "949e2fd6d98e4396bdcbb182007976f1",
      "9e67fee56cf04e92accd63316384bf50",
      "fa1ef6babd3a42d3aff715a1794b4b9f",
      "74a30282572e496e85a3b8510a2d20bd",
      "407845c2229c45bcb8d4ad4a8faa1cdd",
      "6bfa6ec5f7fc4f9d8c58f52105590c46"
     ]
    },
    "id": "fyfR6pswgUEa",
    "outputId": "8ec2cf5e-1c65-4570-8e6b-68d5969385a8"
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "categories = list(directions_dict.keys())\n",
    "category_picker = widgets.Dropdown(options=categories)\n",
    "slider = widgets.IntSlider(value=2, max=4)\n",
    "coeffs = [-5, -1.5, 0, 1.5, 5]\n",
    "\n",
    "\n",
    "category_picker.value\n",
    "display\n",
    "slider.value\n",
    "\n",
    "\n",
    "latent_vector = np.random.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n",
    "\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "latent_vector =  convertZtoW(latent_vector, truncation_psi=1.0)[0]\n",
    "\n",
    "mask = np.zeros(latent_vector.shape, dtype=float)\n",
    "mask[:] = 1\n",
    "button = widgets.Button(description=\"Generate Me!\")\n",
    "button_clear = widgets.Button(description='Clear')\n",
    "\n",
    "def run_button(*args):\n",
    "  coeff = coeffs[slider.value]\n",
    "  global latent_vector\n",
    "  direction = normalize(directions_dict[category_picker.value])\n",
    "  latent_vector = ((latent_vector + coeff*direction*mask))\n",
    "  with out:\n",
    "    image = generate_images_in_w_space(np.expand_dims(latent_vector,0), 1.0, randomize_noise=True, show_progress=False)[0]\n",
    "    IPython.display.clear_output(True)\n",
    "    imshow(image)\n",
    "\n",
    "def truncate(*args):\n",
    "  global latent_vector\n",
    "  truncation_psi = 0.75\n",
    "  dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
    "  latent_vector = (latent_vector - dlatent_avg) * np.reshape(truncation_psi, [1, 1]) + dlatent_avg\n",
    "\n",
    "  with out:\n",
    "      image = generate_images_in_w_space(np.expand_dims(latent_vector,0), 1.0, randomize_noise=True, show_progress=False)[0]\n",
    "\n",
    "      IPython.display.clear_output(True)\n",
    "      imshow(image)\n",
    "\n",
    "  \n",
    "button.on_click(run_button)\n",
    "button_trunc = widgets.Button(description='Truncate')\n",
    "button_trunc.on_click(truncate)\n",
    "display(category_picker, slider, button, button_clear, button_trunc)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcJFmdKpOQs9"
   },
   "source": [
    "# Figures Interpolation\r\n",
    "This creates various interpolation plots of figures in the space. You can use it to go betweeen two seeds you like (interpolates in the Z space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQIhdSRcXC-Q"
   },
   "outputs": [],
   "source": [
    "# generate some random seeds\n",
    "size = 5\n",
    "seeds = np.random.randint(10000000, size=size ** 2)\n",
    "print(seeds)\n",
    "truncation_psi = 1.0\n",
    "\n",
    "# show the seeds\n",
    "imshow(createImageGrid(generate_images_from_seeds(seeds, truncation_psi), 1 , size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwCmDFmhaWP2"
   },
   "source": [
    "#Random Walk Video Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMmqVY76sW9i"
   },
   "source": [
    "This generates a nice grid video of multiple values walking around the latent space. \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "6a99a505306448948392612c12ebae75",
      "5f9f50069b6e4e65970b637313a6044c",
      "a531049aa566408682f31f560f19f3ed",
      "58ed123d3eb04a10a394ece8a5a9363f",
      "8d0eb7ae5c424954838a424076000ef3",
      "6bd3c501fbed41de8eeaf86496c97eb9",
      "b00d955ee999423c893622381a48bf28",
      "a320413a873e48b9af36c5fe65711220"
     ]
    },
    "id": "U6T7h0gpsIJK",
    "outputId": "a77c9c21-2167-418c-d1eb-8c9dcfd32cb9"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "grid_size = [3,3]\n",
    "image_shrink = 1\n",
    "image_zoom = 1\n",
    "duration_sec = 10\n",
    "smoothing_sec = 2.0 # 1.0\n",
    "mp4_fps = 20\n",
    "mp4_codec = 'libx264'\n",
    "mp4_bitrate = '4M'\n",
    "random_seed = np.random.randint(0, 999)#405\n",
    "mp4_file = 'random_grid_%s.mp4' % random_seed\n",
    "minibatch_size = 16\n",
    "truncation_psi= 1.0\n",
    "\n",
    "num_frames = int(np.rint(duration_sec * mp4_fps))\n",
    "random_state = np.random.RandomState(random_seed)\n",
    "\n",
    "# Generate latent vectors\n",
    "shape = [num_frames, np.prod(grid_size)] + Gs.input_shape[1:] # [frame, image, channel, component]\n",
    "all_latents = random_state.randn(*shape).astype(np.float32)\n",
    "all_latents = scipy.ndimage.gaussian_filter(all_latents, [smoothing_sec * mp4_fps] + [0] * len(Gs.input_shape), mode='wrap')\n",
    "all_latents /= np.sqrt(np.mean(np.square(all_latents)))\n",
    "\n",
    "\n",
    "def create_image_grid(images, grid_size=None):\n",
    "    assert images.ndim == 3 or images.ndim == 4\n",
    "    num, img_h, img_w, channels = images.shape\n",
    "\n",
    "    if grid_size is not None:\n",
    "        grid_w, grid_h = tuple(grid_size)\n",
    "    else:\n",
    "        grid_w = max(int(np.ceil(np.sqrt(num))), 1)\n",
    "        grid_h = max((num - 1) // grid_w + 1, 1)\n",
    "\n",
    "    grid = np.zeros([grid_h * img_h, grid_w * img_w, channels], dtype=images.dtype)\n",
    "    for idx in range(num):\n",
    "        x = (idx % grid_w) * img_w\n",
    "        y = (idx // grid_w) * img_h\n",
    "        grid[y : y + img_h, x : x + img_w] = images[idx]\n",
    "    return grid\n",
    "\n",
    "# Frame generation func for moviepy.\n",
    "def make_frame(t):\n",
    "    frame_idx = int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))\n",
    "    latents = all_latents[frame_idx]\n",
    "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    images = Gs.run(latents, None, truncation_psi=truncation_psi,\n",
    "                          randomize_noise=False, output_transform=fmt, \n",
    "                          minibatch_size=minibatch_size)\n",
    "\n",
    "    grid = create_image_grid(images, grid_size)\n",
    "    if image_zoom > 1:\n",
    "        grid = scipy.ndimage.zoom(grid, [image_zoom, image_zoom, 1], order=0)\n",
    "    if grid.shape[2] == 1:\n",
    "        grid = grid.repeat(3, 2) # grayscale => RGB\n",
    "    return grid\n",
    "\n",
    "# Generate video.\n",
    "import moviepy.editor\n",
    "video_clip = moviepy.editor.VideoClip(make_frame, duration=duration_sec)\n",
    "video_clip.write_videofile(mp4_file, fps=mp4_fps, codec=mp4_codec, bitrate=mp4_bitrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "f9b393e643d8420d9b6820f433f4e5b2",
      "096c400d1ca44fe8946eac713647b91d",
      "ac1f52c121ac4ed18fb9c9aa3832b07a",
      "f1a03b1ae66746789e5f1c31d5141f7a",
      "4de263f1562b465080702c754aa185ee",
      "54a6b0cbc9a945b6beb3e83e687bcd13",
      "65f793cbce0741738a9469b5263c71fe",
      "e455eb9d73534dbba982e2dae91b2cb0"
     ]
    },
    "id": "Po7eQSxav8qj",
    "outputId": "78f5cc7a-3365-44e4-e6f8-724f65fea9ae"
   },
   "outputs": [],
   "source": [
    "# In order to download files, you can use the snippet below - this often fails for me, though, so I prefer the 'Files' browser in the sidepanel.\n",
    "\n",
    "from google.colab import files\n",
    "#files.download(mp4_file) \n",
    "show_video(mp4_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jnon7det7L7_"
   },
   "source": [
    "This makes a video where the latent codes are kept constant but the random noise is still applied to the images. This video lets you see the affect of random noise on the output. Like FFHQ, it mostly affects tiny attributes like individual hair strands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "704ff95fe3f24977ab1ff88387b5cc77",
      "cf370bdc48964f8a9a9c498844ea06f0",
      "cedb6d1ed6554ae2b24fde1055f1c350",
      "460c251f88bf49e18bf5997e79b3d7db",
      "8434b96290344bf185aa0d910d072c38",
      "472116760fdf45bbbabab05463669bea",
      "eef1bd18caa9442bab2f050e3cfd9913",
      "3c11d9e3bbca4226beab2f2e48a41743"
     ]
    },
    "id": "q70Wl6RFJ_22",
    "outputId": "056c2213-8727-4dd7-ef10-317bcd07a80b"
   },
   "outputs": [],
   "source": [
    "shape = [num_frames, np.prod(grid_size)] + Gs.input_shape[1:] # [frame, image, channel, component]\n",
    "all_latents = random_state.randn(*shape).astype(np.float32)\n",
    "\n",
    "def make_frame(t):\n",
    "    frame_idx = int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))\n",
    "    latents = all_latents[0]\n",
    "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    images = Gs.run(latents, None, truncation_psi=truncation_psi,\n",
    "                          randomize_noise=True, output_transform=fmt, \n",
    "                          minibatch_size=16)\n",
    "\n",
    "    grid = create_image_grid(images, grid_size)\n",
    "    if image_zoom > 1:\n",
    "        grid = scipy.ndimage.zoom(grid, [image_zoom, image_zoom, 1], order=0)\n",
    "    if grid.shape[2] == 1:\n",
    "        grid = grid.repeat(3, 2) # grayscale => RGB\n",
    "    return grid\n",
    "\n",
    "# Generate video.\n",
    "import moviepy.editor\n",
    "video_clip = moviepy.editor.VideoClip(make_frame, duration=duration_sec)\n",
    "video_clip.write_videofile(mp4_file, fps=mp4_fps, codec=mp4_codec, bitrate=mp4_bitrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "8b7ea985676c4e039419f6e4a1008856",
      "0e5eb31b3c1f46a4b326d889a5bd947b",
      "5d73ea3c48bb46478981f03b9a4b5892",
      "2192d38f08aa4af9af5108952db3fa14",
      "1e5d0a895df74274bb1a6b85d26bd0e7",
      "6aad159837674c41b80b8dd08c5d2a74",
      "a600ffb4b5474b978b57c8186d7c0981",
      "4bd94b1f6fb24f7b855958a7e5dd4799"
     ]
    },
    "id": "c00kc5LUKKfO",
    "outputId": "fd62ea0d-ec61-4bc3-de7f-0a357671646a"
   },
   "outputs": [],
   "source": [
    "# In order to download files, you can use the snippet below - this often fails for me, though, so I prefer the 'Files' browser in the sidepanel.\n",
    "\n",
    "from google.colab import files\n",
    "#files.download(mp4_file) \n",
    "show_video(mp4_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635,
     "referenced_widgets": [
      "209b7eaffa9c42d58f501f0897bc9595",
      "36c6fcecc0604a6cbbf4686674058434",
      "f6d3640fc7a14745b7f7a12606e46221",
      "56d4fcd58a794f339b93fb02a78b0147",
      "784810213b2a4e25a83b1dbe19414ad8",
      "534a85a54fab47d09f8c2a2e84f4e97f",
      "76cfc9a417d2402882be3638572764d0",
      "0d89d982fd184f3a86c534589fb73c3b",
      "bc0dcef921814551b4957685158d14e4",
      "cff08c1960aa4b5787108b03805e65bd",
      "be8523650b0b4b82a192b726568512dc",
      "3061542c5e1a4bcf979dfad69ba38696",
      "73ba17ad59674b85ba87e887e254590d",
      "4abdb0cdcbfe4a89b1cbf7a84098cfc8",
      "67b1d5a1db1f40e784f782da09ef4dea",
      "e5feb97dbb024bd38007bcad05205e17",
      "cb5af386a25c46ed9869a9c089f8d617",
      "b5d3e4a7935546169d801a74d886c5d9",
      "5285d1a1b34347f7b32e130921b44855",
      "bd33fab3c6cb4f7480f338d0b706b11c",
      "bc30167507c449c88229ad84148a1b8c",
      "d1159f2233084a56b06da7933082ce7d",
      "3b3105f3c33c49b2bb3f531c22aa2391",
      "87ab475c00d34c3ea4ea43cc335bd1c8"
     ]
    },
    "id": "TwXUbkVJXckp",
    "outputId": "251c6f0a-8cb4-4c54-ba3d-7739f0a66568"
   },
   "outputs": [],
   "source": [
    "# generating a MP4 movie\n",
    "import moviepy.editor\n",
    "zs = generate_zs_from_seeds([421645,6149575,3487643,3766864 ,3857159,5360657,3720613])\n",
    "\n",
    "number_of_steps = 10\n",
    "imgs = generate_images(interpolate(zs,number_of_steps), 1,0)\n",
    "\n",
    "# Example of reading a generated set of images, and storing as MP4.\n",
    "%mkdir out\n",
    "movieName = 'out/mov.mp4'\n",
    "\n",
    "with imageio.get_writer(movieName, mode='I') as writer:\n",
    "    for image in log_progress(list(imgs), name = \"Creating animation\"):\n",
    "        writer.append_data(np.array(image))\n",
    "show_video(movieName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aZvophLZQOw"
   },
   "outputs": [],
   "source": [
    "# Simple (Z) interpolation\n",
    "zs = generate_zs_from_seeds([401528 , 614808 ])\n",
    "\n",
    "latent1 = zs[0]\n",
    "latent2 = zs[1]\n",
    "\n",
    "number_of_steps = 25\n",
    "truncation_psi = 1.0\n",
    "\n",
    "imgs = generate_images(interpolate([latent1,latent2],number_of_steps), truncation_psi)\n",
    "number_of_images = len(imgs)\n",
    "imshow(createImageGrid(imgs, 1 , 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEy_HbjQ94S4"
   },
   "source": [
    "#Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgdNVt8Vzh6h"
   },
   "source": [
    "##Coarse Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8GHyADJ7ek6"
   },
   "source": [
    "This shows a style transfer of the coarse features (low numbered) feature between a src image and several target images. You can see how these low level features control e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158,
     "referenced_widgets": [
      "801dde1715804702b65ca75f854a48ae",
      "f7bd961ad0324da2be95ef3043c75607",
      "d0bc21dc3eb54056840fd19499eeae00",
      "84ce1ae55fc04d79a7f4322b46b8a99f",
      "41118c5fcec547b5b01dcdb8f8f761e9",
      "badbb05d725b4fbda03148129e6d2016",
      "f3a92adebe8445bfad0118d1c7f577e7",
      "8c52fe2061384c81a3f3c0f231dcb205"
     ]
    },
    "id": "tipAb-M0zkw4",
    "outputId": "568d2927-0bfd-4cbf-bd85-810dfd91cf09"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "duration_sec = 10.0\n",
    "smoothing_sec = 1.0\n",
    "mp4_fps = 20\n",
    "truncation_psi = 1.0\n",
    "\n",
    "num_frames = int(np.rint(duration_sec * mp4_fps))\n",
    "#random_seed = 500\n",
    "random_seed = np.random.randint(0, 999)#405\n",
    "random_state = np.random.RandomState(int(random_seed))\n",
    "\n",
    "\n",
    "h = w = Gs.output_shape[-1]\n",
    "#src_seeds = [601]\n",
    "dst_seeds = [501, 702, 707]\n",
    "num_styles = Gs.components.mapping.output_shape[1]\n",
    "#style_ranges = ([0] * (num_styles // 2) + [range(num_styles // 2, num_styles)]) * len(dst_seeds)\n",
    "style_ranges = (list(range(0, num_styles//2))) * len(dst_seeds)\n",
    "fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "synthesis_kwargs = dict(output_transform=fmt, truncation_psi=truncation_psi, minibatch_size=16)\n",
    "\n",
    "shape = [num_frames] + Gs.input_shape[1:] # [frame, image, channel, component]\n",
    "src_latents = random_state.randn(*shape).astype(np.float32)\n",
    "src_latents = scipy.ndimage.gaussian_filter(src_latents,\n",
    "                                            smoothing_sec * mp4_fps,\n",
    "                                            mode='wrap')\n",
    "src_latents /= np.sqrt(np.mean(np.square(src_latents)))\n",
    "\n",
    "dst_latents = np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in dst_seeds)\n",
    "\n",
    "\n",
    "src_dlatents = Gs.components.mapping.run(src_latents, None) # [seed, layer, component]\n",
    "dst_dlatents = Gs.components.mapping.run(dst_latents, None) # [seed, layer, component]\n",
    "src_images = Gs.components.synthesis.run(src_dlatents, randomize_noise=False, **synthesis_kwargs)\n",
    "dst_images = Gs.components.synthesis.run(dst_dlatents, randomize_noise=False, **synthesis_kwargs)\n",
    "\n",
    "\n",
    "canvas = PIL.Image.new('RGB', (w * (len(dst_seeds) + 1), h * 2), 'white')\n",
    "    \n",
    "for col, dst_image in enumerate(list(dst_images)):\n",
    "    canvas.paste(PIL.Image.fromarray(dst_image, 'RGB'), ((col + 1) * h, 0))\n",
    "\n",
    "def make_frame(t):\n",
    "    frame_idx = int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))\n",
    "    src_image = src_images[frame_idx]\n",
    "    canvas.paste(PIL.Image.fromarray(src_image, 'RGB'), (0, h))\n",
    "    \n",
    "    for col, dst_image in enumerate(list(dst_images)):\n",
    "        col_dlatents = np.stack([dst_dlatents[col]])\n",
    "        col_dlatents[:, style_ranges[col]] = src_dlatents[frame_idx, style_ranges[col]]\n",
    "        col_images = Gs.components.synthesis.run(col_dlatents, randomize_noise=False, **synthesis_kwargs)\n",
    "        for row, image in enumerate(list(col_images)):\n",
    "            canvas.paste(PIL.Image.fromarray(image, 'RGB'), ((col + 1) * h, (row + 1) * w))\n",
    "    return np.array(canvas)\n",
    "    \n",
    "# Generate video.\n",
    "import moviepy.editor\n",
    "mp4_file = 'output.mp4'\n",
    "mp4_codec = 'libx264'\n",
    "mp4_bitrate = '2M'#8M\n",
    "\n",
    "video_clip = moviepy.editor.VideoClip(make_frame, duration=duration_sec)\n",
    "video_clip.write_videofile(mp4_file, fps=mp4_fps, codec=mp4_codec, bitrate=mp4_bitrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 507,
     "referenced_widgets": [
      "68a96f78a8f3409398f57384d6abbfa4",
      "6bc13f68138b4937affebb7833e7cfa7",
      "6fc34a7477404e44839725f711843ec9",
      "717a8fd911f6475a8352afd807b4c36c",
      "b10ebfa25919411187a7e2efe9675db3",
      "14ea127023cf4abd83092ef817aa97fe",
      "0c35399bdb644884b94a603cfea0d866",
      "f9948e42ce7d496b98c6d5e708ff11a5"
     ]
    },
    "id": "0MWhyNPXzodW",
    "outputId": "161fc2eb-c03a-406b-8600-0a49ec6360da"
   },
   "outputs": [],
   "source": [
    "show_video(mp4_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDOY8kUkuJE6"
   },
   "source": [
    "##Fine Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtHaaZyE9F3Y"
   },
   "source": [
    "This code snippit shows fine attribute transfers (defined as the high half of the W latent spaces). These tends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "418588b274b54fb9a53d11b5dffaf6ff",
      "452a6b6d1d1d429baec76da098397da5",
      "26dcd57fd3644e98b0684c02905205ca",
      "9cc15f6ef9634cf7b9dd16b7426be259",
      "b7a1919ed3274ef28927ec9666d38321",
      "7515a7282e874191bc163468cc19df21",
      "bf24b03ebb02486cac847e693d812514",
      "cd6805d8aa3a460d90875a3e14d59ccd"
     ]
    },
    "id": "QnmXCoOBuNGc",
    "outputId": "3060ae2b-86f5-4d86-95aa-b6bb73a0bb0e"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "duration_sec = 20.0\n",
    "smoothing_sec = 1.0\n",
    "mp4_fps = 20\n",
    "\n",
    "num_frames = int(np.rint(duration_sec * mp4_fps))\n",
    "random_seed = 404\n",
    "random_state = np.random.RandomState(random_seed)\n",
    "\n",
    "\n",
    "h = Gs.input_shape[-1]\n",
    "w = h\n",
    "style_num = Gs.components.mapping.output_shape[1]\n",
    "style_ranges = [range(style_num//2, style_num)]\n",
    "\n",
    "fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "synthesis_kwargs = dict(output_transform=fmt, truncation_psi=0.4, minibatch_size=16)\n",
    "\n",
    "shape = [num_frames] + Gs.input_shape[1:] # [frame, image, channel, component]\n",
    "src_latents = random_state.randn(*shape).astype(np.float32)\n",
    "src_latents = scipy.ndimage.gaussian_filter(src_latents,\n",
    "                                            smoothing_sec * mp4_fps,\n",
    "                                            mode='wrap')\n",
    "src_latents /= np.sqrt(np.mean(np.square(src_latents)))\n",
    "\n",
    "dst_latents = np.stack([random_state.randn(Gs.input_shape[1])])\n",
    "\n",
    "\n",
    "src_dlatents = Gs.components.mapping.run(src_latents, None) # [seed, layer, component]\n",
    "dst_dlatents = Gs.components.mapping.run(dst_latents, None) # [seed, layer, component]\n",
    "\n",
    "\n",
    "def make_frame(t):\n",
    "    frame_idx = int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))\n",
    "    col_dlatents = np.stack([dst_dlatents[0]])\n",
    "    col_dlatents[:, style_ranges[0]] = src_dlatents[frame_idx, style_ranges[0]]\n",
    "    col_images = Gs.components.synthesis.run(col_dlatents, randomize_noise=False, **synthesis_kwargs)\n",
    "    return col_images[0]\n",
    "    \n",
    "# Generate video.\n",
    "import moviepy.editor\n",
    "mp4_file = 'fine_%s.mp4' % (random_seed)\n",
    "mp4_codec = 'libx264'\n",
    "mp4_bitrate = '8M'\n",
    "\n",
    "video_clip = moviepy.editor.VideoClip(make_frame, duration=duration_sec)\n",
    "video_clip.write_videofile(mp4_file, fps=mp4_fps, codec=mp4_codec, bitrate=mp4_bitrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IAanM3fuPyf"
   },
   "outputs": [],
   "source": [
    "show_video(mp4_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNCU8yyDOoUY"
   },
   "source": [
    "## Corse and Fine Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8n0LUS8iOEsk"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "duration_sec = 10.0\n",
    "smoothing_sec = 1.0\n",
    "mp4_fps = 20\n",
    "truncation_psi = 0.4\n",
    "\n",
    "num_frames = int(np.rint(duration_sec * mp4_fps))\n",
    "random_seed = np.random.randint(0, 999)#405\n",
    "random_state = np.random.RandomState(int(random_seed))\n",
    "\n",
    "\n",
    "h = w = Gs.output_shape[-1]\n",
    "#src_seeds = [601]\n",
    "dst_seeds = [501, 702, 707]\n",
    "style_num = Gs.components.mapping.output_shape[1]\n",
    "style_ranges = [list(range(0, style_num))] * len(dst_seeds) # ([0] * (style_num // 2) + [range(style_num//2,style_num)]) * len(dst_seeds)\n",
    "\n",
    "fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "synthesis_kwargs = dict(output_transform=fmt, truncation_psi=truncation_psi, minibatch_size=16)\n",
    "\n",
    "shape = [num_frames] + Gs.input_shape[1:] # [frame, image, channel, component]\n",
    "src_latents = random_state.randn(*shape).astype(np.float32)\n",
    "src_latents = scipy.ndimage.gaussian_filter(src_latents,\n",
    "                                            smoothing_sec * mp4_fps,\n",
    "                                            mode='wrap')\n",
    "src_latents /= np.sqrt(np.mean(np.square(src_latents)))\n",
    "\n",
    "dst_latents = np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in dst_seeds)\n",
    "\n",
    "\n",
    "src_dlatents = Gs.components.mapping.run(src_latents, None) # [seed, layer, component]\n",
    "dst_dlatents = Gs.components.mapping.run(dst_latents, None) # [seed, layer, component]\n",
    "src_images = Gs.components.synthesis.run(src_dlatents, randomize_noise=False, **synthesis_kwargs)\n",
    "dst_images = Gs.components.synthesis.run(dst_dlatents, randomize_noise=False, **synthesis_kwargs)\n",
    "\n",
    "\n",
    "canvas = PIL.Image.new('RGB', (w * (len(dst_seeds) + 1), h * 2), 'white')\n",
    "    \n",
    "for col, dst_image in enumerate(list(dst_images)):\n",
    "    canvas.paste(PIL.Image.fromarray(dst_image, 'RGB'), ((col + 1) * h, 0))\n",
    "\n",
    "def make_frame(t):\n",
    "    frame_idx = int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))\n",
    "    src_image = src_images[frame_idx]\n",
    "    canvas.paste(PIL.Image.fromarray(src_image, 'RGB'), (0, h))\n",
    "    \n",
    "    for col, dst_image in enumerate(list(dst_images)):\n",
    "        col_dlatents = np.stack([dst_dlatents[col]])\n",
    "        col_dlatents[:, style_ranges[col]] = src_dlatents[frame_idx, style_ranges[col]]\n",
    "        col_images = Gs.components.synthesis.run(col_dlatents, randomize_noise=False, **synthesis_kwargs)\n",
    "        for row, image in enumerate(list(col_images)):\n",
    "            canvas.paste(PIL.Image.fromarray(image, 'RGB'), ((col + 1) * h, (row + 1) * w))\n",
    "    return np.array(canvas)\n",
    "    \n",
    "# Generate video.\n",
    "import moviepy.editor\n",
    "mp4_file = 'output_all.mp4'\n",
    "mp4_codec = 'libx264'\n",
    "mp4_bitrate = '2M'#8M\n",
    "\n",
    "video_clip = moviepy.editor.VideoClip(make_frame, duration=duration_sec)\n",
    "video_clip.write_videofile(mp4_file, fps=mp4_fps, codec=mp4_codec, bitrate=mp4_bitrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_Kh08RyOcWf"
   },
   "outputs": [],
   "source": [
    "show_video(mp4_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL_Uv2QTFN1w"
   },
   "source": [
    "#Stylemixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f04Y2stEFQCL"
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "from IPython.display import Image, display\n",
    "truncation_psi=0.7\n",
    "fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "synthesis_kwargs = dict(output_transform=fmt, truncation_psi=truncation_psi, minibatch_size=32)\n",
    "h = w = Gs.output_shape[-1]\n",
    "\n",
    "def draw_style_mixing_figure(png, Gs, w, h, src_seeds, dst_seeds, style_ranges):\n",
    "    src_latents = np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in src_seeds)\n",
    "    dst_latents = np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in dst_seeds)\n",
    "    src_dlatents = Gs.components.mapping.run(src_latents, None, truncation_psi=truncation_psi) # [seed, layer, component]\n",
    "    dst_dlatents = Gs.components.mapping.run(dst_latents, None, truncation_psi=truncation_psi) # [seed, layer, component]\n",
    "    src_images = Gs.components.synthesis.run(src_dlatents, randomize_noise=False, **synthesis_kwargs)\n",
    "    dst_images = Gs.components.synthesis.run(dst_dlatents, randomize_noise=False, **synthesis_kwargs)\n",
    "\n",
    "    canvas = PIL.Image.new('RGB', (w * (len(src_seeds) + 1), h * (len(dst_seeds) + 1)), 'white')\n",
    "    for col, src_image in enumerate(list(src_images)):\n",
    "        canvas.paste(PIL.Image.fromarray(src_image, 'RGB'), ((col + 1) * w, 0))\n",
    "    for row, dst_image in enumerate(list(dst_images)):\n",
    "        canvas.paste(PIL.Image.fromarray(dst_image, 'RGB'), (0, (row + 1) * h))\n",
    "        row_dlatents = np.stack([dst_dlatents[row]] * len(src_seeds))\n",
    "        \n",
    "        row_dlatents[:, style_ranges[row]] = src_dlatents[:, style_ranges[row]]\n",
    "        row_images = Gs.components.synthesis.run(row_dlatents, randomize_noise=False, **synthesis_kwargs)\n",
    "        for col, image in enumerate(list(row_images)):\n",
    "            canvas.paste(PIL.Image.fromarray(image, 'RGB'), ((col + 1) * w, (row + 1) * h))\n",
    "    canvas.save(png)\n",
    "    display(Image(png, width=1024))\n",
    "    \n",
    "\n",
    "style_num = Gs.components.mapping.output_shape[1]\n",
    "draw_style_mixing_figure('fig3.png', Gs, w=w, h=h, src_seeds=[634,504,687,606,220], dst_seeds=[406,204,1898,1733,614], style_ranges=[range(0,style_num //3 )]*2+[range(style_num//3,2*style_num//3)]*2+[range(2*style_num//3,style_num)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBJYS-dyEr9W"
   },
   "source": [
    "#MultiRes Image Visualzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMIuYlfAEtX5"
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "from IPython.display import Image, display\n",
    "\n",
    "fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "truncation_psi=0.4\n",
    "synthesis_kwargs = dict(output_transform=fmt, truncation_psi=truncation_psi, minibatch_size=16)\n",
    "h = w = Gs.output_shape[-1]\n",
    "\n",
    "def draw_uncurated_result_figure(png, Gs, cx, cy, cw, ch, rows, lods, seed):\n",
    "    print(png)\n",
    "    latents = np.random.RandomState(seed).randn(sum(rows * 2**lod for lod in lods), Gs.input_shape[1])\n",
    "    images = Gs.run(latents, None, **synthesis_kwargs) # [seed, y, x, rgb]\n",
    "\n",
    "    canvas = PIL.Image.new('RGB', (sum(cw // 2**lod for lod in lods), ch * rows), 'white')\n",
    "    image_iter = iter(list(images))\n",
    "    for col, lod in enumerate(lods):\n",
    "        for row in range(rows * 2**lod):\n",
    "            image = PIL.Image.fromarray(next(image_iter), 'RGB')\n",
    "            image = image.crop((cx, cy, cx + cw, cy + ch))\n",
    "            image = image.resize((cw // 2**lod, ch // 2**lod), PIL.Image.ANTIALIAS)\n",
    "            canvas.paste(image, (sum(cw // 2**lod for lod in lods[:col]), row * ch // 2**lod))\n",
    "    canvas.save(png)\n",
    "    display(Image(png, width=1024))\n",
    "\n",
    "draw_uncurated_result_figure('fig2.png', Gs, cx=0, cy=0, cw=w, ch=h, rows=3, lods=[0,1,2,2,3,3], seed=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GofpNwi5aLl9"
   },
   "outputs": [],
   "source": [
    "# more complex example, interpolating in W instead of Z space.\n",
    "zs = generate_zs_from_seeds([421645,6149575,3487643,3766864 ,3857159,5360657,3720613 ])\n",
    "\n",
    "# It seems my truncation_psi is slightly less efficient in W space - I probably introduced an error somewhere...\n",
    "\n",
    "dls = []\n",
    "for z in zs:\n",
    "  dls.append(convertZtoW(z ,truncation_psi=1.0))\n",
    "\n",
    "number_of_steps = 100\n",
    "\n",
    "imgs = generate_images_in_w_space(interpolate(dls,number_of_steps), 1.0)\n",
    "\n",
    "%mkdir out\n",
    "movieName = 'out/mov.mp4'\n",
    "\n",
    "with imageio.get_writer(movieName, mode='I') as writer:\n",
    "    for image in log_progress(list(imgs), name = \"Creating animation\"):\n",
    "        writer.append_data(np.array(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXRNSH97kZpj"
   },
   "outputs": [],
   "source": [
    "show_video(movieName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "7pH9q_7MzP2T",
    "outputId": "a91544cf-040e-4d39-c9b6-6a0b1f92a586"
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image, display\n",
    "\n",
    "def draw_truncation_trick_figure(png, Gs, w=512, h=512, seeds=[91, 81, 388], psis=[1, 0.7, 0.5, 0, -0.5, -1]):\n",
    "    #print(png)\n",
    "    latents = np.stack(np.random.RandomState(seed).randn(Gs.input_shape[1]) for seed in seeds)\n",
    "    dlatents = Gs.components.mapping.run(latents, None) # [seed, layer, component]\n",
    "    dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
    "\n",
    "    canvas = PIL.Image.new('RGB', (w * len(psis), h * len(seeds)), 'white')\n",
    "    for row, dlatent in enumerate(list(dlatents)):\n",
    "        row_dlatents = (dlatent[np.newaxis] - dlatent_avg) * np.reshape(psis, [-1, 1, 1]) + dlatent_avg\n",
    "        row_images = Gs.components.synthesis.run(row_dlatents, randomize_noise=False, **synthesis_kwargs)\n",
    "        for col, image in enumerate(list(row_images)):\n",
    "            canvas.paste(PIL.Image.fromarray(image, 'RGB'), (col * w, row * h))\n",
    "            \n",
    "    canvas.save(png)\n",
    "    IPython.display.display(IPython.display.Image(png, width=1024))\n",
    "        #PIL.Image(png, width=1024))\n",
    "h = w = Gs.output_shape[-1]\n",
    "draw_truncation_trick_figure('output.png', Gs, w=w, h=h, seeds=[901, 888,777], psis=[2.0, 1, 0.7, 0.5, 0, -0.5, -0.7 -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYdsgv4i6YPl"
   },
   "source": [
    "# (Bonus) Projecting images onto the generatable manifold\n",
    "\n",
    "StyleGAN2 comes with a projector that finds the closest generatable image based on any input image. This allows you to get a feeling for the diversity of the portrait manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urzy8lw76j_r"
   },
   "outputs": [],
   "source": [
    "!mkdir projection\n",
    "!mkdir projection/imgs\n",
    "!mkdir projection/out\n",
    "\n",
    "# Now upload a single image to 'stylegan2/projection/imgs' (use the Files side panel). Image should be color PNG, with a size of 1024x1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "IDLJBbpz6n4k",
    "outputId": "e4f4a365-7b66-4cd5-cac7-1eb8507d74c3"
   },
   "outputs": [],
   "source": [
    "# Convert uploaded images to TFRecords\n",
    "import dataset_tool\n",
    "# You have to upload images for this to work\n",
    "dataset_tool.create_from_images(\"./projection/records/\", \"./projection/imgs/\", True)\n",
    "!rm 'projection/records/-r10.tfrecords'\n",
    "\n",
    "# Run the projector\n",
    "import run_projector\n",
    "import projector\n",
    "import training.dataset\n",
    "import training.misc\n",
    "import os \n",
    "import cv2\n",
    "\n",
    "def project_real_images(dataset_name, data_dir, num_images, num_snapshots):\n",
    "    proj = projector.Projector()\n",
    "    proj.set_network(Gs)\n",
    "\n",
    "    print('Loading images from \"%s\"...' % dataset_name)\n",
    "    dataset_obj = training.dataset.load_dataset(data_dir=data_dir, tfrecord_dir=dataset_name, max_label_size=0, verbose=True, repeat=False, shuffle_mb=0)\n",
    "    print(dataset_obj.shape)\n",
    "    print(Gs.output_shape)\n",
    "    assert dataset_obj.shape == Gs.output_shape[1:]\n",
    "\n",
    "    for image_idx in range(num_images):\n",
    "        print('Projecting image %d/%d ...' % (image_idx, num_images))\n",
    "        images, _labels = dataset_obj.get_minibatch_np(1)\n",
    "        images = training.misc.adjust_dynamic_range(images, [0, 255], [-1, 1])\n",
    "        run_projector.project_image(proj, targets=images, png_prefix=dnnlib.make_run_dir_path('projection/out/image%04d-' % image_idx), num_snapshots=num_snapshots)\n",
    "\n",
    "project_real_images(\"records\",\"./projection\",1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmjPpjFU6yq3"
   },
   "outputs": [],
   "source": [
    "# Create video \n",
    "\n",
    "import glob\n",
    "\n",
    "imgs = sorted(glob.glob(\"projection/out/*step*.png\"))\n",
    "\n",
    "target_imgs = sorted(glob.glob(\"projection/out/*target*.png\"))\n",
    "assert len(target_imgs) == 1, \"More than one target found?\"\n",
    "target_img = imageio.imread(target_imgs[0])\n",
    "\n",
    "movieName = \"projection/movie.mp4\"\n",
    "with imageio.get_writer(movieName, mode='I') as writer:\n",
    "    for filename in log_progress(imgs, name = \"Creating animation\"):\n",
    "        image = imageio.imread(filename)\n",
    "\n",
    "        # Concatenate images with original target image\n",
    "        w,h = image.shape[0:2]\n",
    "        canvas = PIL.Image.new('RGBA', (w*2,h), 'white')\n",
    "        canvas.paste(Image.fromarray(target_img), (0, 0))\n",
    "        canvas.paste(Image.fromarray(image), (w, 0))\n",
    "\n",
    "        writer.append_data(np.array(canvas))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGVarLre63dL"
   },
   "outputs": [],
   "source": [
    "# Now you can download the video (find it in the Files side panel under 'stylegan2/projection')\n",
    "\n",
    "# To cleanup\n",
    "!rm projection/out/*.*\n",
    "!rm projection/records/*.*\n",
    "!rm projection/imgs/*.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "StyleGAN of Anime Sliders by Skyli0n v0",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
